{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquistion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the fixed size for images\n",
    "fixed_size = (100, 100)  # Change this to your desired size\n",
    "max_images_per_class = 100  # Change this to the maximum number of images per class\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = './Traffic Sign/DATA'\n",
    "\n",
    "# Initialize a dictionary to store images\n",
    "data_dict = {}\n",
    "\n",
    "# Function to perform image augmentation\n",
    "def augment_images(image):\n",
    "    augmented_images = []\n",
    "    \n",
    "    # Perform rotation\n",
    "    for angle in range(-15, 16, 5):\n",
    "        rotated = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "        augmented_images.append(cv2.resize(rotated, fixed_size))\n",
    "        \n",
    "    # Perform flipping\n",
    "    flipped_lr = cv2.flip(image, 1)\n",
    "    flipped_ud = cv2.flip(image, 0)\n",
    "    augmented_images.extend([cv2.resize(flipped_lr, fixed_size), cv2.resize(flipped_ud, fixed_size)])\n",
    "    \n",
    "    # Perform scaling\n",
    "    for scale in np.linspace(0.9, 1.1, 5):\n",
    "        scaled = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        augmented_images.append(cv2.resize(scaled, fixed_size))\n",
    "        \n",
    "    return augmented_images\n",
    "\n",
    "# Pad or crop images to ensure uniform length\n",
    "def pad_or_crop_images(images):\n",
    "    max_length = max(len(img) for img in images)\n",
    "    padded_images = []\n",
    "    for img in images:\n",
    "        diff = max_length - len(img)\n",
    "        if diff > 0:\n",
    "            # Pad the image\n",
    "            padding = [(diff // 2, diff // 2 + diff % 2), (0, 0), (0, 0)]\n",
    "            padded_img = np.pad(img, padding, mode='constant')\n",
    "        else:\n",
    "            # Crop the image\n",
    "            cropped_img = img[:max_length]\n",
    "        padded_images.append(padded_img if diff > 0 else cropped_img)\n",
    "    return padded_images\n",
    "\n",
    "# Iterate through each folder (class)\n",
    "for filename in os.listdir(folder_path):\n",
    "    image_list = []\n",
    "    curr_path = os.path.join(folder_path, filename)\n",
    "    count = 0  # Counter to limit the number of images per class\n",
    "    \n",
    "    # Read images from each class folder\n",
    "    for imagename in os.listdir(curr_path):\n",
    "        if count >= max_images_per_class:\n",
    "            break\n",
    "        image = cv2.imread(os.path.join(curr_path, imagename))\n",
    "        \n",
    "        # Augment images\n",
    "        augmented_images = augment_images(image)\n",
    "        image_list.extend(augmented_images)\n",
    "        \n",
    "        count += 1  # Increment the counter\n",
    "        \n",
    "    # Pad or crop images to ensure uniform length\n",
    "    image_list = pad_or_crop_images(image_list)\n",
    "        \n",
    "    # Store the augmented images for each class\n",
    "    data_dict[filename] = image_list\n",
    "\n",
    "# Now data_dict contains augmented images for each class, all with uniform length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "1400\n",
      "1400\n",
      "1400\n",
      "1400\n",
      "1400\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "for key in data_dict:\n",
    "    print(len(data_dict[key]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to apply normalization, histogram equalization, and denoising\n",
    "def preprocess_image(image):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Normalization\n",
    "    normalized_image = cv2.normalize(gray_image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    \n",
    "    # Histogram Equalization\n",
    "    equalized_image = cv2.equalizeHist(gray_image)\n",
    "    \n",
    "    # Denoising\n",
    "    denoised_image = cv2.fastNlMeansDenoising(equalized_image, None, h=10, searchWindowSize=21)\n",
    "    \n",
    "    return denoised_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define the fixed size for images\n",
    "fixed_size = (100, 100)  # Change this to your desired size\n",
    "max_images_per_class = 100  # Change this to the maximum number of images per class\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = './Traffic Sign/DATA'\n",
    "\n",
    "# Initialize a dictionary to store preprocessed images\n",
    "preprocessed_data_dict = {}\n",
    "\n",
    "# Function to perform image augmentation\n",
    "def augment_images(image):\n",
    "    augmented_images = []\n",
    "    \n",
    "    # Perform rotation\n",
    "    for angle in range(-15, 16, 5):\n",
    "        rotated = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "        augmented_images.append(cv2.resize(rotated, fixed_size))\n",
    "        \n",
    "    # Perform flipping\n",
    "    flipped_lr = cv2.flip(image, 1)\n",
    "    flipped_ud = cv2.flip(image, 0)\n",
    "    augmented_images.extend([cv2.resize(flipped_lr, fixed_size), cv2.resize(flipped_ud, fixed_size)])\n",
    "    \n",
    "    # Perform scaling\n",
    "    for scale in np.linspace(0.9, 1.1, 5):\n",
    "        scaled = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        augmented_images.append(cv2.resize(scaled, fixed_size))\n",
    "        \n",
    "    return augmented_images\n",
    "\n",
    "# Pad or crop images to ensure uniform length\n",
    "def pad_or_crop_images(images):\n",
    "    max_length = max(len(img) for img in images)\n",
    "    padded_images = []\n",
    "    for img in images:\n",
    "        diff = max_length - len(img)\n",
    "        if diff > 0:\n",
    "            # Pad the image\n",
    "            padding = [(diff // 2, diff // 2 + diff % 2), (0, 0), (0, 0)]\n",
    "            padded_img = np.pad(img, padding, mode='constant')\n",
    "        else:\n",
    "            # Crop the image\n",
    "            cropped_img = img[:max_length]\n",
    "        padded_images.append(padded_img if diff > 0 else cropped_img)\n",
    "    return padded_images\n",
    "\n",
    "\n",
    "# Preprocessing function to apply normalization, histogram equalization, and denoising\n",
    "def preprocess_image(image):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Normalization\n",
    "    normalized_image = cv2.normalize(gray_image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    \n",
    "    # Histogram Equalization\n",
    "    equalized_image = cv2.equalizeHist(gray_image)\n",
    "    \n",
    "    # Denoising\n",
    "    denoised_image = cv2.fastNlMeansDenoising(equalized_image, None, h=10, searchWindowSize=21)\n",
    "    \n",
    "    return denoised_image\n",
    "\n",
    "# Iterate through each folder (class)\n",
    "for filename in os.listdir(folder_path):\n",
    "    image_list = []\n",
    "    curr_path = os.path.join(folder_path, filename)\n",
    "    count = 0  # Counter to limit the number of images per class\n",
    "    \n",
    "    # Read images from each class folder\n",
    "    for imagename in os.listdir(curr_path):\n",
    "        if count >= max_images_per_class:\n",
    "            break\n",
    "        image = cv2.imread(os.path.join(curr_path, imagename))\n",
    "        \n",
    "        # Preprocess image\n",
    "        preprocessed_image = preprocess_image(image)\n",
    "        \n",
    "        # Augment images\n",
    "        augmented_images = augment_images(preprocessed_image)\n",
    "        image_list.extend(augmented_images)\n",
    "        \n",
    "        count += 1  # Increment the counter\n",
    "        \n",
    "    # Pad or crop images to ensure uniform length\n",
    "    image_list = pad_or_crop_images(image_list)\n",
    "        \n",
    "    # Store the augmented and preprocessed images for each class\n",
    "    preprocessed_data_dict[filename] = image_list\n",
    "\n",
    "# Now preprocessed_data_dict contains augmented and preprocessed images for each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Input image is empty.\n",
      "Error: Input image is empty.\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "\n",
    "# # Function to compute HOG features\n",
    "# def compute_hog_features(image):\n",
    "#     if image is None:\n",
    "#         print(\"Error: Input image is empty.\")\n",
    "#         return None\n",
    "    \n",
    "#     # Convert image to grayscale\n",
    "#     gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "#     # Compute HOG features\n",
    "#     hog = cv2.HOGDescriptor()\n",
    "#     hog_features = hog.compute(gray_image)\n",
    "    \n",
    "#     return hog_features\n",
    "\n",
    "# # Function to compute LBP features\n",
    "# def compute_lbp_features(image):\n",
    "#     if image is None:\n",
    "#         print(\"Error: Input image is empty.\")\n",
    "#         return None\n",
    "    \n",
    "#     # Convert image to grayscale\n",
    "#     gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "#     # Compute LBP features\n",
    "#     lbp = cv2.LBP_create()\n",
    "#     lbp_features = lbp.compute(gray_image)\n",
    "    \n",
    "#     return lbp_features\n",
    "\n",
    "# # Load an example image\n",
    "# image = cv2.imread('example_image.jpg')\n",
    "\n",
    "# # Compute HOG features\n",
    "# hog_features = compute_hog_features(image)\n",
    "\n",
    "# # Compute LBP features\n",
    "# lbp_features = compute_lbp_features(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, let's break down each method:\n",
    "\n",
    "### Histogram of Oriented Gradients (HOG):\n",
    "\n",
    "HOG is a feature descriptor that captures the distribution of gradient orientations in an image. It's widely used in object detection and classification tasks. Here's how you can implement it using OpenCV and NumPy:\n",
    "\n",
    "1. **Convert Image to Grayscale**: HOG works on grayscale images. So, the first step is to convert the input color image to grayscale.\n",
    "   \n",
    "2. **Compute Gradients**: Compute the gradients (both magnitude and orientation) of the grayscale image using techniques like Sobel operator.\n",
    "   \n",
    "3. **Divide Image into Cells**: Divide the image into small cells, typically 8x8 pixels.\n",
    "   \n",
    "4. **Compute Histograms**: For each cell, compute a histogram of gradient orientations. These histograms capture the distribution of gradient orientations within each cell.\n",
    "   \n",
    "5. **Block Normalization**: Normalize the histograms within each block (a group of cells). This normalization helps in achieving invariance to changes in illumination and contrast.\n",
    "   \n",
    "6. **Concatenate Block Features**: Concatenate the normalized block features to form the final feature vector for the image.\n",
    "\n",
    "OpenCV provides a built-in function `cv2.HOGDescriptor()` to compute HOG features.\n",
    "\n",
    "### Local Binary Patterns (LBP):\n",
    "\n",
    "LBP is another feature descriptor used for texture classification. It encodes the local structure of an image by comparing each pixel with its neighbors. Here's how you can implement it using OpenCV and NumPy:\n",
    "\n",
    "1. **Convert Image to Grayscale**: Similar to HOG, LBP also operates on grayscale images.\n",
    "   \n",
    "2. **Compute LBP for Each Pixel**: For each pixel in the image, compare its intensity value with the intensity values of its neighbors. Encode the result as a binary number.\n",
    "   \n",
    "3. **Histogram of LBP**: Compute a histogram of the LBP values over the entire image or its regions. This histogram represents the distribution of local patterns in the image.\n",
    "   \n",
    "4. **Concatenate Histograms**: Concatenate the histograms from different regions (if applicable) to form the final feature vector for the image.\n",
    "\n",
    "OpenCV provides a function `cv2.calcHist()` to compute histograms, which can be used to compute the LBP histogram.\n",
    "\n",
    "### Utilizing Pre-trained CNNs for Feature Extraction:\n",
    "\n",
    "Deep CNNs, especially those pretrained on large datasets like ImageNet, learn rich hierarchical features that are useful for various computer vision tasks. Here's how you can extract features from pre-trained CNNs using TensorFlow's Keras API:\n",
    "\n",
    "1. **Load Pre-trained Model**: Load a pre-trained CNN model such as VGG, ResNet, or Inception from TensorFlow's Keras API. These models are trained on large datasets and capture rich image representations.\n",
    "   \n",
    "2. **Remove Classification Head**: Remove the fully connected layers (classification head) from the pre-trained model. We're interested in the features learned by the convolutional layers.\n",
    "   \n",
    "3. **Feature Extraction**: Pass the input images through the modified pre-trained model and extract features from one of the intermediate layers. These features serve as the representation of the input images.\n",
    "   \n",
    "4. **Use Extracted Features**: The extracted features can be used directly for classification tasks or as input to other machine learning models.\n",
    "\n",
    "TensorFlow's Keras API provides an easy way to load pre-trained models and extract features using the `predict()` method.\n",
    "\n",
    "### Comparison and Reasons for Classification Task:\n",
    "\n",
    "- **HOG and LBP**: These methods are suitable for tasks where local texture and shape information are important, such as object detection and texture classification. They are computationally efficient and provide interpretable features. However, they may not capture high-level semantic information present in deep CNN features.\n",
    "\n",
    "- **Pre-trained CNNs**: Pre-trained CNN features are suitable for tasks where high-level semantic information is crucial, such as image classification, object recognition, and scene understanding. They capture rich, hierarchical representations learned from large-scale datasets. However, they may require more computational resources for both training and inference.\n",
    "\n",
    "The choice between these methods depends on the specific requirements of the classification task, including the nature of the input data, computational constraints, and desired level of interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VGG16\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg16\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_input\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Define the layers from which to extract features\n",
    "feature_extractor = tf.keras.Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)\n",
    "\n",
    "# Function to extract features from images using pre-trained CNN\n",
    "def extract_cnn_features(images):\n",
    "    # Preprocess images\n",
    "    preprocessed_images = []\n",
    "    for image in images:\n",
    "        preprocessed_image = preprocess_input(image)  # Preprocess input images\n",
    "        preprocessed_images.append(preprocessed_image)\n",
    "    preprocessed_images = np.array(preprocessed_images)\n",
    "    \n",
    "    # Extract features\n",
    "    features = feature_extractor.predict(preprocessed_images)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage:\n",
    "# Assuming img_dict is a 2D dictionary containing RGB images in the array\n",
    "\n",
    "# Extract features from images\n",
    "extracted_features = {}\n",
    "for category, images in dict.items():\n",
    "    features = extract_cnn_features(images)\n",
    "    extracted_features[category] = features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
